+++
# A Demo section created with the Blank widget.
# Any elements can be added in the body: https://sourcethemes.com/academic/docs/writing-markdown-latex/
# Add more sections by duplicating this file and customizing to your requirements.

widget = "blank"  # See https://sourcethemes.com/academic/docs/page-builder/
headless = true  # This file represents a page section.
active = true  # Activate this widget? true/false
weight = 30  # Order that this section will appear.

title = "Train Your Engineering Network"

# Hero image (optional). Enter filename of an image in the `static/img/` folder.
hero_media = ""

[design]
  # Choose how many columns the section has. Valid values: 1 or 2.
  columns = "1"

  
[design.background]
  # Apply a background color, gradient, or image.
  #   Uncomment (by removing `#`) an option to apply it.
  #   Choose a light or dark text color by setting `text_color_light`.
  #   Any HTML color name or Hex value is valid.

  # Background color.
  color = ""
  
  # Background gradient.
  # gradient_start = "DeepSkyBlue"
  # gradient_end = "SkyBlue"
  
  # Background image.
  image = ""  # Name of image in `static/img/`.
  image_darken = 0.5  # Darken the image? Range 0-1 where 0 is transparent and 1 is opaque.
  image_size = "actual"  #  Options are `cover` (default), `contain`, or `actual` size.
  image_position = "center"  # Options include `left`, `center` (default), or `right`.
  image_parallax = false  # Use a fun parallax-like fixed background effect? true/false

  # Text color (true=light or false=dark).
  text_color_light = false

[design.spacing]
  # Customize the section spacing. Order is top, right, bottom, left.
  padding = ["30px", "0", "30px", "0"]



+++

### Ziel:
Die Vortragsreihe "Train your engineering network" zu vielfältigen Themen des Machine Learnings wendet sich an alle interessierten Personen an der TUHH, von MLE-Partnern sowie allgemein aus der Region Hamburg und zielt darauf ab, den Informationsaustausch und Wissenstransfer zwischen diesen Personen sowie deren Vernetzung in lockerer Atmosphäre zu fördern. Dadurch sollen die Machine-Learning-Aktivitäten innerhalb von MLE, der TUHH sowie im weiteren Umfeld sichtbarer gemacht, Kooperationen gefördert und auch interessierten Studierenden ein Einblick ermöglicht werden.

**English version:** *The presentation series "Train your engineering network" on diverse topics of Machine Learning addresses all interested persons at TUHH, from MLE partners as well as from the Hamburg region in general and aims at promoting the exchange of information and knowledge between these persons as well as their networking in a relaxed atmosphere. Thereby, the machine learning activities within MLE, TUHH and in the wider environment shall be made more visible, cooperations shall be promoted and also interested students shall be given an insight.*

### Ansprechpartner:
Organisatoren sind Mijail Guillemard, {{% mention "kraeuter" %}}, {{% mention "vonbun_feldbauer" %}}, {{% mention "zemke" %}}.

### Ort und Zeit:
Die Vorträge finden im Wintersemester 2023 online über Zoom montags ab 16:00 in englischer Sprache statt.
Allgemeiner Zoom Link für alle Vorträge: [Link](https://tuhh.zoom.us/j/87239759122?pwd=bWhXMUUvUnhFRUI4WkdxRTZsVDZFdz09)

**English version:** *Lectures will be held online via Zoom on Mondays starting at 16:00 in the winter semester 2023 in English.
General zoom link for all lectures: [Link](https://tuhh.zoom.us/j/87239759122?pwd=bWhXMUUvUnhFRUI4WkdxRTZsVDZFdz09)*

### Inhalte und Vortragende im aktuellen Semester:

| # | Datum | Zeit | Vortragende/r | Thema |
| --- | --- | --- | --- | --- |
| 1 | 16.10.23 | 16:00&nbsp;-&nbsp;17:00 | Bernhard&nbsp;Berger | Machine Learning in Optimisation with Applications to Material Science [(Video)](https://webcast.tu-harburg.de/Mediasite/Play/94898395887642aea87368cf4aaab7341d) |
| 2 | 23.10.23 | 16:00&nbsp;-&nbsp;17:00 | {{% mention "schwarz_h" %}} | Comparison of LSTM and Koopman-Operator approaches for Predicting Transient Ditching Loads [(Video)](https://webcast.tu-harburg.de/Mediasite/Play/5c3522f5aadc4bea94a455d75c1e9bc41d) |
| 3 | 30.10.23 | 16:00&nbsp;-&nbsp;17:00 | Ana&nbsp;Almeida | Multivariate Time series: Data processing, Imputation and Forecasting  |
| 4 | 06.11.23 | 16:00&nbsp;-&nbsp;17:00 | {{% mention "itin" %}} | AI for engineering and science: selected use cases |
| 5 | 13.11.23 | 16:00&nbsp;-&nbsp;17:00 | {{% mention "saleh" %}} | Flow-induced bases and application to quantum molecular physics |
| 6 | 20.11.23 | 16:00&nbsp;-&nbsp;17:00 | {{% mention "schibsdat" %}} & Denys&nbsp;Romanenko | Self-acting anomaly detection and quality estimation for semi-automated drilling with machine learning methods |
| 7 | 27.11.23 | 16:00&nbsp;-&nbsp;17:00 | {{% mention "braun" %}} | Generalizability and explainability of machine learning models for fatigue strength prediction of welded joints |
| 8 | 04.12.23 | 16:00&nbsp;-&nbsp;17:00 | {{% mention "ibrahim" %}} | Parareal with a physics informed neural network as a coarse propagator |
| 9 | 11.12.23 | 16:00&nbsp;-&nbsp;17:00 | {{% mention "kraeuter" %}} | Development of a black-box soft sensor for a fluidization process |
| 10 | 18.12.23 | 16:00&nbsp;-&nbsp;17:00 | tba | tba |
| - | 25.12.23 | - | - | Holiday - Merry Christmas! |
| - | 01.01.24 | - | - | Holiday - Happy New Year! |
| 11 | 08.01.24 | 16:00&nbsp;-&nbsp;17:00 | {{% mention "roeder" %}} | Hindsight Instruction Grounding in Reinforcement Learning |
| 12 | 15.01.24 | 16:00&nbsp;-&nbsp;17:00 | Lars&nbsp;Stietz | tba |
| 13 | 22.01.24 | 16:00&nbsp;-&nbsp;17:00 | Emin&nbsp;Nakilcioglu | Parameter Efficient Fine Tuning for a Domain-Specific Automatic Speech Recognition |
| 14 | 29.01.24 | 16:00&nbsp;-&nbsp;17:00 | Niklas&nbsp;Dieckow | Data-driven methods for the Maxey-Riley equations |


#### Abstracts:

<details class="description" close><summary data-close="Show" data-open="Hide"></summary>

1. {{< hl >}}Bernhard Berger: Machine Learning in Optimisation with Applications to Material Science.{{< /hl >}} <br/>
Many real-world projects aim at finding optimal solutions to a specific problem and search space. The optimization task can be hard in itself, but often the problem function is not even known. In such cases, it is necessary to experimentally test possible solutions for their appropriateness. In many domains, such as material science, it is expensive and time-consuming to do these tests. Therefore, ML is a technique to bridge this gap and give hints on the performance of a proposed solution. In this talk, I will delve into the problem of surrogate functions, how they can be learned, and how their prediction quality can be used to steer the optimisation process. I will demonstrate this approach using EvoAl, a DSL-based optimisation framework.

2. {{< hl >}}{{% mention "schwarz_h" %}}: Comparison of LSTM and Koopman-Operator approaches for Predicting Transient Ditching Loads.{{< /hl >}} <br/>
This research is concerned with building machine learning (ML) models to predict dynamic ditching loads on aircraft fuselages. The employed learning procedure is structured into two parts, the reconstruction of the spatial loads using a convolutional autoencoder (CAE) and the transient evolution of these loads in a subsequent part. Both parts are simultaneously (jointly) learned in a global network. To predict transient load evolution, the CAE is combined with either different long short-term memory (LSTM) networks or a Koopman-operator based method. To this end, both approaches advance the solution in time based on information from two previous and the present time step. The training data is compiled by applying an extension of the momentum method of von-Karman and Wagner to simulate the loads on a generic DLR-D150 fuselage model at various approach conditions. Results indicate that both baseline methods, i.e., the LSTM and the Koopman-based approach, are able to perform accurate ditching load predictions. Predictive differences occur when looking at the different options to capture the temporal evolution of loads and will be outlined in greater detail. 

3. {{< hl >}}Ana Almeida: Multivariate Time series: Data processing, Imputation and Forecasting.{{< /hl >}} <br/>
Data is a valuable tool for decision-makers, helping them make informed decisions. We can find multivariate time series in several contexts, such as finances, smart cities, and health. This type of data can bring additional challenges. This presentation will discuss the key concepts and techniques involved in working with multivariate time series data. Specifically, we will focus on the steps of data processing, imputation, and forecasting.

4. {{< hl >}}{{% mention "itin" %}}: AI for engineering and science: selected use cases.{{< /hl >}} <br/>
Its going to be a little bit chaotic talk with many use cases.  My scientific background is dynamical systems and condensed matter physics.  I switched from academic to industrial research at some point, joining Bosch Research, and did many interesting projects there. I then gradually returned back to academy. I will briefly discuss applications of AI in engineering, and in a more detail in science: how they are different and what have in common.  Such use cases as virtual sensors, synthetic FIB/SEM data generation, anomaly detection in manufacturing, etc, will be briefly reviewed (engineering). I will consider in a little bit more detail inverse design of photonic structures using generative AI, predictive physics-informed models, acceleration of solvers using AI  (science).

5. {{< hl >}}{{% mention "saleh" %}}: Flow-induced bases and application to quantum molecular physics.{{< /hl >}} <br/>
In analogy to the use of normalizing flows to augment the expressivity of base probability distributions, I propose to augment the expressivity of bases of Hilbert spaces via composition with normalizing flows. I show that the redsulting sequences are also bases of the Hilbert space under sufficient and necessary conditions on the flow. This lays a foundation for a theory of spectral learning, a nonlinear extension of spectral methods for solving differential equations. As an application I solve the vibrational molecular Schrödinger equation. The proposed numerical scheme results in several orders of magnitude increased accuracy over the use of standard spectral methods. 

6. {{< hl >}} {{% mention "schibsdat" %}} & Denys Romanenko: Self-acting anomaly detection and quality estimation for semi-automated drilling with machine learning methods.{{< /hl >}} <br/>
Due to the high number of rivet holes per aircraft produced, automated process monitoring of the drilling process promises a significant reduction in manual inspection.
Advances in sensor technology in new machine tools are greatly expanding the data base. Thus, self-learning can be applied to holistic process monitoring.   
In this presentation, the authors present approaches to anomaly detection and quality control in the drilling process. Supervised, semi-supervised and unsupervised methods were used for anomaly detection and compared with classical methods of quality control charts. In addition to engineered feature extraction, a new method was used to extract features using a CNN. For the prediction of the quality of the parts, different methods of classification and regression were compared, giving different results in terms of prediction quality.

7. {{< hl >}}{{% mention "braun" %}}: Generalizability and explainability of machine learning models for fatigue strength prediction of welded joints.{{< /hl >}} <br/>
Fatigue is the main cause of structural failure of large engineering structures. Welds, with their geometry leading to high local stresses, are especially vulnerable. Traditional fatigue assessment methods, which factor in material properties, load levels, and idealized weld geometries, can be inaccurate. To address this, data-driven approaches, using machine learning (ML) algorithms and 3D-laser scanners for weld geometry, have been successful in predicting fatigue life for butt-welded joints; however, it remains uncertain whether these methods are adaptable to different welding techniques and welds with imperfections. This presentation addresses the generalizability of machine learning approaches for fatigue strength assessment for welded joints by assessing data, which differs from the training dataset in various ways. The new data contains results for a different welding procedure, and of welded joints with imperfections and weld defects. By comparing prediction accuracies between the original data and the new data, the study aims to determine the adaptability of the data-driven approach to new, divergent data. The focus is on assessing how anomalous weld geometries impact prediction accuracy, ultimately establishing the limitations of applying this method to varying data. To this goal, explainable artificial intelligence is applied.

8. {{< hl >}}{{% mention "ibrahim" %}}: Parareal with a physics informed neural network as a coarse propagator.{{< /hl >}} <br/>
Parallel-in-time algorithms provide an additional layer of concurrency for the numerical integration of models based on time-dependent differential equations. Methods like Parareal, which parallelize across multiple time steps, rely on a computationally cheap and coarse integrator to propagate information forward in time, while a parallelizable expensive fine propagator provides accuracy. Typically, the coarse method is a numerical integrator using lower resolution, reduced order or a simplified model. Our paper proposes to use a physics-informed neural network (PINN) instead. We demonstrate for the Black-Scholes equation, a partial differential equation from computational finance, that Parareal with a PINN coarse propagator provides better speedup than a numerical coarse propagator. Training and evaluating a neural network are both tasks whose computing patterns are well suited for GPUs. By contrast, mesh-based algorithms with their low computational intensity struggle to perform well. We show that moving the coarse propagator PINN to a GPU while running the numerical fine propagator on the CPU further improves Parareal's single-node performance. This suggests that integrating machine learning techniques into parallel-in-time integration methods and exploiting their differences in computing patterns might offer a way to better utilize heterogeneous architectures. 

9. {{< hl >}}{{% mention "kraeuter" %}}: Development of a black-box soft sensor for a fluidization process.{{< /hl >}} <br/>
Solids water content is an important particle property in many applications of process engineering. Its importance on the quality of pharmaceutical formulations makes an in-line measurement of the water content especially desirable in fluidization processes. However, currently available measurement techniques are difficult to calibrate and scarcely applicable in real fluidized beds. A promising strategy for in-line monitoring of the water content is thus soft sensing, a method that expresses the targeted quantity as a correlation of other more reliable measurements.
In this talk, we present the development of such a soft sensor using various black-box models. Our focus lies on strategies to reduce overfitting through feature engineering and hyperparameter tuning. These models are designed for processing real experimental data from a turbulent process, addressing challenges in data filtering, undersampling, outlier detection, and uncertainty propagation.


10. {{< hl >}}tba: tba{{< /hl >}} <br/>
tba

11. {{< hl >}}{{% mention "roeder" %}}: Hindsight Instruction Grounding in Reinforcement Learning.{{< /hl >}} <br/>
This presentation addresses the challenge of sample inefficiency in robotic reinforcement learning with sparse rewards and natural language goal representations. We introduce a mechanism for hindsight instruction replay, leveraging expert feedback, and a seq2seq model for generating linguistic hindsight instructions. Remarkably, our findings demonstrate that self-supervised language generation, where the agent autonomously generates linguistic instructions, significantly enhances learning performance. These results underscore the promising potential of hindsight instruction grounding in reinforcement learning for robotics.

12. {{< hl >}}Lars Stietz: tba.{{< /hl >}} <br/>
tba

13. {{< hl >}}Emin Nakilcioglu: Parameter Efficient Fine Tuning for a Domain-Specific Automatic Speech Recognition.{{< /hl >}} <br/>
With the introduction of early pre-trained language models such as Google’s BERT and various early GPT models, we have seen an ever-increasing excitement and interest in foundation models. To leverage existing pre-trained foundation models and adapt them to specific tasks or domains, these models need to be fine-tuned using domain-specific data. However, fine-tuning can be quite resource-intensive and costly as millions of parameters will be modified as part of training.   
PEFT is a technique designed to fine-tune models while minimizing the need for extensive resources and cost. It achieves this efficiency by freezing some of the layers of the pre-trained model and only fine-tuning the last few layers that are specific to the downstream task. With the help of PEFT, we can achieve a balance between retaining valuable knowledge from the pre-trained model and adapting it effectively to the downstream task with fewer parameters.

14. {{< hl >}}Niklas Dieckow: Data-driven methods for the Maxey-Riley equations.{{< /hl >}} <br/>
The Maxey-Riley equations (MRE) describe the motion of a small inertial particle suspended in a fluid flow. They are a system of implicit integro-differential equations with a singular kernel. Exact solution methods require the evaluation of an integral over the entire particle history in each time step, causing the computation time to grow quadratically in the number of steps. In this talk, data-driven methods such as SINDy (Sparse Identification of Nonlinear Dynamics) are discussed and employed to obtain approximations of the MRE that do not contain an integral term and are therefore easier to solve.

</details>

### Vergangene Semester:
 * [Sommersemester 2022](/_archive/network/#sose22)
 * [Wintersemester 2021/22](/_archive/network/#wise21_22)
 * [Sommersemester 2021](/_archive/network/#sose21)
 * [Wintersemester 2020/21](/_archive/network/#wise20_21)
 * [Sommersemester 2020](/_archive/network/#sose20)
